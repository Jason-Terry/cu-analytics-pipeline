# Skyla Credit Union Analytics Pipeline

A demo data pipeline that generates a fake credit union database with PII, scrubs it through an ETL pipeline using Microsoft Presidio for validation, and serves AI-powered analytics via a FastAPI + Claude API.

## Architecture

```
credit_union.db (PII)  -->  ETL Pipeline  -->  analytics.db (scrubbed)
                              |                       |
                        member_link.db           FastAPI API
                     (member_id <-> analytics_id)     |
                                                 Claude Sonnet
                                                      |
                                              JSON / CSV+Charts ZIP
```

**Three databases, strict separation:**

| Database | Contains | Who can access |
|----------|----------|----------------|
| `credit_union.db` | Real PII (names, SSN, email, phone, addresses) | Source system only |
| `member_link.db` | member_id <-> analytics_id mapping | Admin lookup only |
| `analytics.db` | Scrubbed data (age brackets, tiers, regions, opaque IDs) | Claude / API / analysts |

The LLM never sees `credit_union.db` or `member_link.db`. SQL generated by Claude is executed through a read-only, sandboxed SQLite connection that only allows SELECT on `members_clean` and `loans_clean`.

## Setup

### Requirements

- Python 3.10+
- An [Anthropic API key](https://console.anthropic.com/)

### Install dependencies

```bash
pip install -e .
```

Then install the spaCy language model (required by Presidio):

```bash
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl
```

### Configure environment

```bash
cp .env.example .env
# Edit .env and add your Anthropic API key
```

### Build the databases

**1. Seed the source database** (generates fake PII data):

```bash
python3 scripts/seed_db.py
```

This creates `data/credit_union.db` with 10,000 members and ~8,700 loans. To seed a larger dataset:

```python
# In scripts/seed_db.py, change num_members:
seed_database(conn, num_members=435_000)
```

**2. Run the ETL pipeline** (scrubs PII, builds analytics.db + member_link.db):

```bash
python3 scripts/run_etl.py
```

This will:
- Generate opaque `analytics_id` for each member (12-char hex)
- Store member_id <-> analytics_id mappings in `member_link.db`
- Transform members: drop PII, convert DOB -> age bracket, credit score -> tier, state -> region
- Transform loans: replace member_id with analytics_id, drop origination date -> year only
- Validate schema (no PII columns in output)
- Run Presidio NLP on a 1% sample of clean records as a safety check

### Start the API

```bash
python3 -m uvicorn src.api.main:app --port 8000
```

## API Endpoints

### ETL

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/etl/trigger` | Start an ETL run in background |
| GET | `/api/etl/status` | Check ETL pipeline status |

### Analytics

All analytics endpoints accept `?format=json` (default) or `?format=report` (returns a ZIP with CSVs, PNG charts, and an AI summary).

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/analytics/portfolio` | Loan portfolio analysis |
| POST | `/api/analytics/demographics` | Member demographics analysis |
| POST | `/api/analytics/delinquency` | Loan delinquency analysis |
| POST | `/api/analytics/query` | Custom natural language question |

### Custom query example

```bash
curl -X POST http://localhost:8000/api/analytics/query \
  -H "Content-Type: application/json" \
  -d '{"question": "How many members joined in 2020?"}'
```

Custom queries use a three-pass Claude pipeline:
1. **Generate SQL** from the natural language question
2. **Review agent** validates the SQL logic and results
3. **Interpret** the results in plain language

### Download a report

```bash
curl -X POST "http://localhost:8000/api/analytics/portfolio?format=report" -o portfolio.zip
```

## Project Structure

```
src/
  db/
    schema.py       # Table definitions for all 3 databases
    seed.py         # Faker-based data generator
  etl/
    pipeline.py     # ETL orchestrator (parallel, batched)
    scrubber.py     # Presidio validation + helper transforms
  api/
    main.py         # FastAPI app setup
    routes.py       # Endpoint definitions
    analyst.py      # Claude-powered analytics + SQL sandboxing
    reports.py      # CSV + matplotlib chart generation
    logger.py       # Logging with timing support
scripts/
  seed_db.py        # CLI: seed the source database
  run_etl.py        # CLI: run the ETL pipeline
```

## Security

- **Structural PII exclusion**: The ETL physically cannot copy PII fields. Names, SSN, email, phone, and addresses are never written to analytics.db.
- **Pseudonymization**: Real member IDs are replaced with opaque analytics_ids (UUID4 hex). The mapping lives in `member_link.db`, which is never exposed to the API or LLM.
- **SQL sandboxing**: LLM-generated SQL is validated against a blocklist (no ATTACH, CREATE, DROP, etc.) and executed through a read-only SQLite connection with an authorizer that restricts access to only `members_clean` and `loans_clean`.
- **Presidio validation**: NLP-based PII detection runs on a sample of clean records as a secondary safety net.

## Data Transformations

The ETL converts raw PII fields into analytics-safe equivalents. No PII crosses into the analytics database â€” only bucketed, generalized values.

| Source Field (PII) | Analytics Field | Transformation |
|--------------------|-----------------|----------------|
| `member_id` | `analytics_id` | Replaced with opaque 12-char UUID4 hex |
| `date_of_birth` | `age_bracket` | Bucketed: 18-25, 26-35, 36-45, 46-55, 56-65, 65+ |
| `credit_score` | `credit_score_range` | Bucketed into 5 loan approval tiers (Tier 1-5) |
| `state` | `state` + `region` | State kept, region derived (Northeast, South, Midwest, West) |
| `membership_date` | `membership_year` + `tenure_years` | Date dropped, year and tenure calculated |
| `origination_date` | `origination_year` | Full date dropped, year only |
| first_name, last_name, ssn, email, phone, address, city, zip | *(dropped)* | Never written to analytics.db |

## Presidio False Positive Handling

Presidio runs NLP-based PII detection on a sample of the clean output as a validation layer. However, some legitimate analytics fields trigger false positives:

- **State abbreviations** (e.g. `"CA"`, `"NY"`) flagged as `LOCATION`
- **Age brackets** (e.g. `"26-35"`) flagged as `DATE_TIME`
- **Analytics IDs** (random hex like `"99971fe3a4f4"`) flagged as `DATE_TIME`, `PERSON`, `LOCATION`, or `MEDICAL_LICENSE`

These are handled via an `EXPECTED_ENTITIES` allowlist in `src/etl/scrubber.py`:

```python
EXPECTED_ENTITIES: dict[str, set[str]] = {
    "analytics_id": {"DATE_TIME", "PERSON", "LOCATION", "MEDICAL_LICENSE"},
    "state": {"LOCATION"},
    "region": {"LOCATION"},
    "age_bracket": {"DATE_TIME"},
    "credit_score_range": {"DATE_TIME"},
}
```

When Presidio flags a field, the validator checks: is this entity type expected for this field? If yes, it's a known false positive and gets skipped. Only unexpected findings (e.g. a real SSN pattern appearing in a field that shouldn't have one) are reported.
