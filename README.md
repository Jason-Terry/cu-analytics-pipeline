# Skyla Credit Union Analytics Pipeline

A demo data pipeline that generates a fake credit union database with PII, scrubs it through an ETL pipeline using Microsoft Presidio for validation, and serves AI-powered analytics via a FastAPI + Claude API.

## Architecture

```
credit_union.db (PII)  -->  ETL Pipeline  -->  analytics.db (scrubbed)
                              |                       |
                        member_link.db           FastAPI API
                     (member_id <-> analytics_id)     |
                                                 Claude Sonnet
                                                      |
                                              JSON / CSV+Charts ZIP
```

**Three databases, strict separation:**

| Database | Contains | Who can access |
|----------|----------|----------------|
| `credit_union.db` | Real PII (names, SSN, email, phone, addresses) | Source system only |
| `member_link.db` | member_id <-> analytics_id mapping | Admin lookup only |
| `analytics.db` | Scrubbed data (age brackets, tiers, regions, opaque IDs) | Claude / API / analysts |

The LLM never sees `credit_union.db` or `member_link.db`. SQL generated by Claude is executed through a read-only, sandboxed SQLite connection that only allows SELECT on `members_clean` and `loans_clean`.

## Setup

### Requirements

- Python 3.10+
- An [Anthropic API key](https://console.anthropic.com/)

### Install dependencies

```bash
pip install -e .
```

Then install the spaCy language model (required by Presidio):

```bash
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl
```

### Configure environment

```bash
cp .env.example .env
# Edit .env and add your Anthropic API key
```

### Build the databases

**1. Seed the source database** (generates fake PII data):

```bash
python3 scripts/seed_db.py
```

This creates `data/credit_union.db` with 10,000 members and ~8,700 loans. To seed a larger dataset:

```python
# In scripts/seed_db.py, change num_members:
seed_database(conn, num_members=435_000)
```

**2. Run the ETL pipeline** (scrubs PII, builds analytics.db + member_link.db):

```bash
python3 scripts/run_etl.py
```

This will:
- Generate opaque `analytics_id` for each member (12-char hex)
- Store member_id <-> analytics_id mappings in `member_link.db`
- Transform members: drop PII, convert DOB -> age bracket, credit score -> tier, state -> region
- Transform loans: replace member_id with analytics_id, drop origination date -> year only
- Validate schema (no PII columns in output)
- Run Presidio NLP on a 1% sample of clean records as a safety check

### Start the API

```bash
python3 -m uvicorn src.api.main:app --port 8000
```

## API Endpoints

### ETL

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/etl/trigger` | Start an ETL run in background |
| GET | `/api/etl/status` | Check ETL pipeline status |

### Analytics

All analytics endpoints accept `?format=json` (default) or `?format=report` (returns a ZIP with CSVs, PNG charts, and an AI summary).

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/analytics/portfolio` | Loan portfolio analysis |
| POST | `/api/analytics/demographics` | Member demographics analysis |
| POST | `/api/analytics/delinquency` | Loan delinquency analysis |
| POST | `/api/analytics/query` | Custom natural language question |

### Custom query example

```bash
curl -X POST http://localhost:8000/api/analytics/query \
  -H "Content-Type: application/json" \
  -d '{"question": "How many members joined in 2020?"}'
```

Custom queries use a three-pass Claude pipeline:
1. **Generate SQL** from the natural language question
2. **Review agent** validates the SQL logic and results
3. **Interpret** the results in plain language

### Download a report

```bash
curl -X POST "http://localhost:8000/api/analytics/portfolio?format=report" -o portfolio.zip
```

## Project Structure

```
src/
  db/
    schema.py       # Table definitions for all 3 databases
    seed.py         # Faker-based data generator
  etl/
    pipeline.py     # ETL orchestrator (parallel, batched)
    scrubber.py     # Presidio validation + helper transforms
  api/
    main.py         # FastAPI app setup
    routes.py       # Endpoint definitions
    analyst.py      # Claude-powered analytics + SQL sandboxing
    reports.py      # CSV + matplotlib chart generation
    logger.py       # Logging with timing support
scripts/
  seed_db.py        # CLI: seed the source database
  run_etl.py        # CLI: run the ETL pipeline
```

## Security

- **Structural PII exclusion**: The ETL physically cannot copy PII fields. Names, SSN, email, phone, and addresses are never written to analytics.db.
- **Pseudonymization**: Real member IDs are replaced with opaque analytics_ids (UUID4 hex). The mapping lives in `member_link.db`, which is never exposed to the API or LLM.
- **SQL sandboxing**: LLM-generated SQL is validated against a blocklist (no ATTACH, CREATE, DROP, etc.) and executed through a read-only SQLite connection with an authorizer that restricts access to only `members_clean` and `loans_clean`.
- **Presidio validation**: NLP-based PII detection runs on a sample of clean records as a secondary safety net.

## Credit Tiers

| Tier | Score Range | Description |
|------|-------------|-------------|
| Tier 1 | 741+ | Super-prime, best terms |
| Tier 2 | 671-740 | Prime, competitive rates |
| Tier 3 | 581-670 | Near-prime, standard rates |
| Tier 4 | 451-580 | Subprime, higher rates |
| Tier 5 | 450 and below | Highest risk, limited products |
